import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Mxfp4Config, Trainer, TrainingArguments, default_data_collator
from peft import LoraConfig, get_peft_model
from datasets import DatasetDict
from dataclasses import dataclass
from transformers import DataCollatorForLanguageModeling # ğŸ‘ˆ æ·»åŠ è¿™ä¸€è¡Œ
# import matplotlib.pyplot as plt # 1. å¯¼å…¥ç»˜å›¾åº“


dataset = load_dataset("/home_data/home/ligx2025/tmp/t25_112/multilingual_thinking_local", split="train")
tokenizer = AutoTokenizer.from_pretrained("/public_bme2/bme-wangqian2/lgx2025/gpt-oss-20b") 

# --- â¬‡ï¸ åœ¨è¿™é‡Œæ·»åŠ æ–°ä»£ç  â¬‡ï¸ ---
if tokenizer.pad_token is None:
    print("Tokenizer did not have a pad_token, setting it to eos_token.")
    tokenizer.pad_token = tokenizer.eos_token
# --- â¬†ï¸ æ·»åŠ ç»“æŸ â¬†ï¸ ---
    
messages = dataset[0]["messages"]
conversation = tokenizer.apply_chat_template(messages, tokenize=False)
print(conversation)

quantization_config = Mxfp4Config(dequantize=True)
model_kwargs = dict(
    attn_implementation="eager",
    torch_dtype=torch.bfloat16,
    quantization_config=quantization_config,
    use_cache=False,
    device_map="auto",
)

model = AutoModelForCausalLM.from_pretrained("/public_bme2/bme-wangqian2/lgx2025/gpt-oss-20b", **model_kwargs)

peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules="all-linear",
    target_parameters=[
        # MoE ä¸“å®¶å±‚çš„æŠ•å½±ï¼ŒæŒ‰éœ€å¢å‡
        "7.mlp.experts.gate_up_proj",
        "7.mlp.experts.down_proj",
        "15.mlp.experts.gate_up_proj",
        "15.mlp.experts.down_proj",
        "23.mlp.experts.gate_up_proj",
        "23.mlp.experts.down_proj",
    ]
)
peft_model = get_peft_model(model, peft_config)
# --- â¬‡ï¸ åœ¨è¿™é‡Œæ·»åŠ æ–°ä»£ç  â¬‡ï¸ ---
peft_model.enable_input_require_grads()
# --- â¬†ï¸ æ·»åŠ ç»“æŸ â¬†ï¸ ---
peft_model.print_trainable_parameters()

# max_length = 4096
max_length = 16

def format_and_tokenize(example):
    # æœŸæœ›å­˜åœ¨ "messages" å­—æ®µï¼ˆå’Œä½ ç¤ºä¾‹ä¸€è‡´ï¼‰
    messages = example["messages"]
    # ä¸åŠ  generation_promptï¼›è®©æ¨¡å‹å­¦ä¹ åˆ°å®Œæ•´çš„å¯¹è¯å±•å¼€
    text = tokenizer.apply_chat_template(
        messages, tokenize=False
    )
    # ç›´æ¥æ•´ä½“ tokenizationï¼Œlabels=inputsï¼ˆç”± collator å¤„ç†ï¼‰
    tokens = tokenizer(
        text,
        truncation=True,
        max_length=max_length,
        return_attention_mask=True,
    )
    return tokens

tokenized = dataset.map(format_and_tokenize, remove_columns=dataset.column_names)
# ç®€å•åˆ’ä¸ªéªŒè¯é›†ï¼ˆå¯é€‰ï¼‰
splits = tokenized.train_test_split(test_size=0.01, seed=42)
train_ds, eval_ds = splits["train"], splits["test"]

# @dataclass
# class CausalDataCollator:
#     tokenizer: AutoTokenizer
#     mlm: bool = False # è¿™ä¸ªå‚æ•°åœ¨è¿™é‡Œæ²¡ç”¨ï¼Œæ˜¯ä¸ºMLMæ¨¡å‹ï¼ˆå¦‚BERTï¼‰å‡†å¤‡çš„
#     def __call__(self, features):
#         # 1. ä½¿ç”¨é»˜è®¤ collator å°†åˆ—è¡¨ä¸­çš„ feature (å­—å…¸) è½¬æ¢æˆ Pytorch å¼ é‡
#         # æ¯”å¦‚ï¼ŒæŠŠ 4 ä¸ª [512] çš„ input_ids åˆ—è¡¨ï¼Œå †å æˆ [4, 512] çš„å¼ é‡
#         batch = default_data_collator(features)
#         # 2. è¿™æ˜¯å› æœè¯­è¨€æ¨¡å‹ (Causal LM) è®­ç»ƒçš„å…³é”®
#         if "labels" not in batch:
#             # 3. å°† input_ids å¤åˆ¶ä¸€ä»½ä½œä¸º labels
#             batch["labels"] = batch["input_ids"].clone()
#         return batch
'''
ä»€ä¹ˆæ˜¯å› æœè¯­è¨€æ¨¡å‹ (Causal LM)ï¼Ÿ æ¨¡å‹çš„ç›®æ ‡æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚ä¾‹å¦‚ï¼Œç»™å®š "A B C"ï¼Œæ¨¡å‹éœ€è¦é¢„æµ‹ "B C D"ã€‚

ä¸ºä»€ä¹ˆ labels = input_idsï¼Ÿ åœ¨ Trainer å†…éƒ¨ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨å°† labels å‘å³å¹³ç§»ä¸€ä½ã€‚

è¾“å…¥ (input_ids)ï¼š [<bos>, token_A, token_B, token_C]

æ ‡ç­¾ (labels)ï¼š [token_A, token_B, token_C, <eos>] (ç”± Trainer è‡ªåŠ¨å¤„ç†å¹³ç§»å’Œæ©ç )

ä½œç”¨ï¼š è¿™ç¡®ä¿äº†æ¨¡å‹åœ¨è®­ç»ƒæ—¶ï¼Œæ˜¯æ‹¿ input_ids å»é¢„æµ‹ labelsï¼Œè¿™æ­£æ˜¯ GPT è¿™ç±»æ¨¡å‹ï¼ˆCausal LMï¼‰çš„æ ‡å‡†è®­ç»ƒæ–¹å¼ã€‚

'''

# collator = CausalDataCollator(tokenizer)
collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# training_args = TrainingArguments(
#     output_dir="/public_bme2/bme-wangqian2/lgx2025/gpt-oss-20b-multilingual-reasoner",
#     per_device_train_batch_size=4,
#     gradient_accumulation_steps=4,
#     num_train_epochs=1.0,
#     learning_rate=2e-4,
#     lr_scheduler_type="cosine_with_min_lr",
#     lr_scheduler_kwargs={"min_lr_rate": 0.1},
#     warmup_ratio=0.03,
#     logging_steps=1,
#     save_steps=200,
#     save_total_limit=2,
#     bf16=True,
#     gradient_checkpointing=True,
#     report_to=[],
# )

training_args = TrainingArguments(
    output_dir="/public_bme2/bme-wangqian2/lgx2025/gpt-oss-20b-multilingual-reasoner",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    num_train_epochs=1.0,
    learning_rate=2e-4,
    lr_scheduler_type="cosine_with_min_lr",
    lr_scheduler_kwargs={"min_lr_rate": 0.1},
    warmup_ratio=0.03,
    logging_steps=1,
    save_steps=200,
    save_total_limit=2,
    bf16=True,
    gradient_checkpointing=True,
    report_to=[],
)

trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=eval_ds,
    tokenizer=tokenizer,
    data_collator=collator,
)

trainer.train()

# # --- 2. æ·»åŠ ç»˜å›¾åŠŸèƒ½ ---

# print("æ­£åœ¨ç»˜åˆ¶ Loss æ›²çº¿...")
# # ä» trainer.state.log_history ä¸­æå–è®­ç»ƒæ—¥å¿—
# logs = trainer.state.log_history
# train_logs = [log for log in logs if 'loss' in log] # ç­›é€‰è®­ç»ƒæ—¥å¿—
# eval_logs = [log for log in logs if 'eval_loss' in log] # ç­›é€‰è¯„ä¼°æ—¥å¿—

# # æå–è®­ç»ƒ loss å’Œ steps
# train_steps = [log['step'] for log in train_logs]
# train_losses = [log['loss'] for log in train_logs]

# # æå–è¯„ä¼° loss å’Œ steps
# eval_steps = [log['step'] for log in eval_logs]
# eval_losses = [log['eval_loss'] for log in eval_logs]

# # --- ç»˜åˆ¶å›¾åƒ ---
# plt.figure(figsize=(10, 6))

# # ç»˜åˆ¶è®­ç»ƒ Loss
# plt.plot(train_steps, train_losses, label='Training Loss')

# # ç»˜åˆ¶è¯„ä¼° Loss
# # ç”¨ 'o-' æ ·å¼è®©è¯„ä¼°ç‚¹æ›´æ¸…æ™°
# if eval_steps: # ç¡®ä¿æœ‰è¯„ä¼°æ—¥å¿—
#     plt.plot(eval_steps, eval_losses, 'o-', label='Evaluation Loss', markersize=4)

# plt.title('Training and Evaluation Loss Curve')
# plt.xlabel('Steps')
# plt.ylabel('Loss')
# plt.legend()
# plt.grid(True) # æ·»åŠ ç½‘æ ¼

# # 3. ä¿å­˜å›¾åƒ
# output_dir = training_args.output_dir
# plot_path = f"{output_dir}/loss_curve.png"
# plt.savefig(plot_path)

# print(f"Loss æ›²çº¿å·²ä¿å­˜è‡³: {plot_path}")